### 1. Proactor (io_uring) vs Reactor (Muduo) 模块设计异同

虽然两者都采用了 **One Loop Per Thread** 的并发模型，但在核心模块的职责和交互上有显著差异。

| 模块              | Muduo (Reactor/epoll)                                                   | Proactor (io_uring)                                                                         | 核心差异点                                                          |
| :---------------- | :---------------------------------------------------------------------- | :------------------------------------------------------------------------------------------ | :------------------------------------------------------------------ |
| **EventLoop**     | 核心是 `epoll_wait`。负责监听 fd 的**可读/可写状态**。                  | 核心是 `io_uring_wait_cqe`。负责获取**异步操作的完成结果**。                                | **状态 vs 结果**。Reactor 告诉你“能读了”，Proactor 告诉你“读完了”。 |
| **Channel**       | 封装 fd 和感兴趣的事件 (EPOLLIN/OUT)。是 Loop 和 fd 的桥梁。            | **不需要 Channel**。通常直接将 `TcpConnection` 或 `Context` 指针绑定到 SQE 的 `user_data`。 | Proactor 不需要维护“关注列表”，每次操作都是一次性的提交。           |
| **TcpConnection** | `handleRead` 中调用 `read()` 系统调用。`handleWrite` 中调用 `write()`。 | `asyncRead` 提交 `IORING_OP_READ`。`asyncWrite` 提交 `IORING_OP_WRITE`。                    | **同步调用 vs 异步提交**。Proactor 的读写不阻塞，也不立即发生。     |
| **Buffer**        | 读：从内核 socket 读到 Buffer。写：从 Buffer 拷到内核 socket。          | **读：必须预先提供 Buffer 给内核**。写：提交 Buffer 指针给内核。                            | **Buffer 生命周期管理**。Proactor 必须保证 I/O 完成前 Buffer 有效。 |
| **Acceptor**      | 监听 listenfd 的可读事件，调用 `accept()`。                             | 提交 `IORING_OP_ACCEPT` 请求。                                                              | 同样是异步的。                                                      |

---

### 2. 主从线程分发实现方案 (Main Proactor -> Sub Proactor)

实现的是经典的 **Main-Sub Proactor** 模型。

**目标**：
1.  **主线程 (Main Loop)**：只负责 `Accept` 新连接。
2.  **从属线程 (Sub Loop)**：负责处理已连接 Socket 的读写。
3.  **分发**：主线程拿到新连接 `fd` 后，把它“扔”给某个子线程。

#### 核心难点：跨线程任务分发
在 `io_uring` 中，子线程通常阻塞在 `io_uring_wait_cqe` 等待 I/O 完成。主线程不能直接操作子线程的 `io_uring` 实例（虽然 `io_uring` 是线程安全的，但多线程竞争同一个 Ring 性能极差，且逻辑复杂）。

**最佳实践方案：使用 `eventfd` 唤醒子线程**

#### 实现步骤

1.  **EventLoop 增强**：
    每个 `EventLoop` 内部持有一个 `eventfd`。
    *   初始化时，提交一个针对 `eventfd` 的 `IORING_OP_READ` 请求。
    *   当 `eventfd` 可读时，说明有主线程派发的任务来了。

2.  **任务队列**：
    每个 `EventLoop` 维护一个线程安全的任务队列 `std::vector<Functor> pendingFunctors_`。

3.  **分发逻辑 (TcpServer::newConnection)**：
    *   主线程 `Acceptor` 获得新连接 `client_fd`。
    *   主线程选择一个子线程 `io_loop` (Round-Robin)。
    *   主线程调用 `io_loop->runInLoop(cb)`。
    *   `cb` 是一个 Lambda，内容是：`new TcpConnection(io_loop, client_fd, ...)`。

4.  **唤醒逻辑 (EventLoop::runInLoop)**：
    *   将 `cb` 放入子线程的 `pendingFunctors_`。
    *   往子线程的 `eventfd` 写入 8 字节数据。
    *   子线程的 `io_uring_wait_cqe` 返回（因为 `eventfd` 读完成了）。
    *   子线程处理完 CQE 后，检查 `pendingFunctors_` 并执行 `cb`。
    *   `cb` 执行时，创建 `TcpConnection`，并立即提交第一个 `IORING_OP_READ` 请求。

#### 代码示意

**EventLoop.hpp (伪代码)**
```cpp
class EventLoop {
public:
    void runInLoop(Functor cb) {
        if (isInLoopThread()) {
            cb();
        } else {
            queueInLoop(cb);
        }
    }

    void queueInLoop(Functor cb) {
        {
            std::lock_guard<std::mutex> lock(mutex_);
            pendingFunctors_.push_back(cb);
        }
        wakeup(); // 写 eventfd
    }

    void wakeup() {
        uint64_t one = 1;
        write(wakeupFd_, &one, sizeof(one)); // 触发 io_uring 返回
    }
    
    // ... loop() 中处理完 CQE 后，执行 doPendingFunctors()
};
```

**TcpServer.cpp (分发)**
```cpp
void TcpServer::newConnection(int sockfd, const InetAddress& peerAddr) {
    EventLoop* ioLoop = threadPool_->getNextLoop();
    
    // 将创建连接的任务派发给子线程
    ioLoop->runInLoop([=](){
        // 此时已在子线程中运行
        TcpConnection::Ptr conn(new TcpConnection(ioLoop, sockfd, ...));
        connections_[connName] = conn;
        conn->connectEstablished(); // 提交第一个 asyncRead
    });
}
```

这种设计保证了 **One Loop Per Thread** 的原则：每个 `io_uring` 实例只被一个线程操作，避免了锁竞争，性能最高。

---

## 3. 主从线程任务分发机制 - 详细实现解析

### 3.1 整体架构概览图

```
                            ┌─────────────────────────────────────────┐
                            │              Main Thread                │
                            │         (Acceptor EventLoop)            │
                            │                                         │
                            │  ┌─────────────────────────────────┐    │
                            │  │         io_uring                │    │
                            │  │   ┌───────────────────────┐     │    │
                            │  │   │   Accept 请求         │     │    │
                            │  │   └───────────────────────┘     │    │
                            │  └─────────────────────────────────┘    │
                            │                  │                      │
                            │                  │ 新连接到来            │
                            │                  ▼                      │
                            │         newConnection()                 │
                            │                  │                      │
                            └──────────────────┼──────────────────────┘
                                               │
                 ┌─────────────────────────────┼─────────────────────────────┐
                 │                             │                             │
                 │         Round-Robin 轮询选择 IO 线程                       │
                 │                             │                             │
                 ▼                             ▼                             ▼
    ┌──────────────────────┐    ┌──────────────────────┐    ┌──────────────────────┐
    │    IO Thread 1       │    │    IO Thread 2       │    │    IO Thread N       │
    │   (Sub EventLoop)    │    │   (Sub EventLoop)    │    │   (Sub EventLoop)    │
    │                      │    │                      │    │                      │
    │  ┌────────────────┐  │    │  ┌────────────────┐  │    │  ┌────────────────┐  │
    │  │   io_uring     │  │    │  │   io_uring     │  │    │  │   io_uring     │  │
    │  │ ┌────────────┐ │  │    │  │ ┌────────────┐ │  │    │  │ ┌────────────┐ │  │
    │  │ │ Read/Write │ │  │    │  │ │ Read/Write │ │  │    │  │ │ Read/Write │ │  │
    │  │ └────────────┘ │  │    │  │ └────────────┘ │  │    │  │ └────────────┘ │  │
    │  └────────────────┘  │    │  └────────────────┘  │    │  └────────────────┘  │
    │                      │    │                      │    │                      │
    │  ┌────────────────┐  │    │  ┌────────────────┐  │    │  ┌────────────────┐  │
    │  │  eventfd       │  │    │  │  eventfd       │  │    │  │  eventfd       │  │
    │  │  (wakeup)      │  │    │  │  (wakeup)      │  │    │  │  (wakeup)      │  │
    │  └────────────────┘  │    │  └────────────────┘  │    │  └────────────────┘  │
    └──────────────────────┘    └──────────────────────┘    └──────────────────────┘
```

### 3.2 核心组件职责一览

| 组件                  | 职责                                           | 所属线程       |
|----------------------|------------------------------------------------|---------------|
| `TcpServer`          | 服务器入口，管理连接生命周期                      | 主线程        |
| `Acceptor`           | 监听端口，接收新连接                             | 主线程        |
| `EventLoopThreadPool`| 管理 IO 线程池，轮询分配连接                     | 主线程        |
| `EventLoopThread`    | 封装 IO 线程及其 EventLoop                      | 从线程        |
| `EventLoop`          | 事件循环，封装 io_uring，处理 I/O 完成事件        | 每个线程各一个 |
| `TcpConnection`      | 封装已建立的 TCP 连接，管理读写缓冲区             | 从线程        |

---

### 3.3 服务器启动阶段 - 函数调用链

```
TcpServer::start()
    │
    ├─▶ EventLoopThreadPool::start()
    │       │
    │       ├─▶ 循环创建 numThreads_ 个 EventLoopThread
    │       │       │
    │       │       └─▶ EventLoopThread::startLoop()
    │       │               │
    │       │               ├─▶ std::thread 创建新线程
    │       │               │       │
    │       │               │       └─▶ threadFunc()  [在新线程中执行]
    │       │               │               │
    │       │               │               ├─▶ EventLoop loop; (栈上创建)
    │       │               │               │
    │       │               │               ├─▶ mutex_.lock()
    │       │               │               ├─▶ loop_ = &loop;
    │       │               │               ├─▶ cond_.notify_one()
    │       │               │               ├─▶ mutex_.unlock()
    │       │               │               │
    │       │               │               └─▶ loop.loop()  [进入事件循环]
    │       │               │
    │       │               └─▶ cond_.wait() 阻塞等待 loop_ 被设置
    │       │                       │
    │       │                       └─▶ 返回 loop_ 指针
    │       │
    │       └─▶ loops_.push_back(loop)  收集所有 EventLoop 指针
    │
    └─▶ acceptor_->listen()
```

**关键代码实现 - EventLoopThread::threadFunc():**
```cpp
void EventLoopThread::threadFunc()
{
    EventLoop loop;  // 在子线程栈上创建 EventLoop
    
    if (callback_) {
        callback_(&loop);  // 执行初始化回调
    }

    {
        std::unique_lock<std::mutex> lock(mutex_);
        loop_ = &loop;       // 设置指针
        cond_.notify_one();  // 通知主线程
    }

    loop.loop();  // 开始事件循环（阻塞）

    std::lock_guard<std::mutex> lock(mutex_);
    loop_ = nullptr;
}
```

---

### 3.4 新连接处理阶段 - 完整调用链

当客户端连接到来时，处理流程如下：

```
                    ┌─────────────────────────────────────┐
                    │           主线程 (Main Thread)       │
                    └─────────────────────────────────────┘
                                       │
                                       │ io_uring CQE: Accept 完成
                                       ▼
                    ┌─────────────────────────────────────┐
                    │ EventLoop::handleCompletionEvent()  │
                    │     ↓                               │
                    │ Acceptor::handleRead(connfd)        │
                    │     ↓                               │
                    │ newConnectionCallback_(connfd, addr)│
                    └─────────────────────────────────────┘
                                       │
                                       ▼
                    ┌─────────────────────────────────────┐
                    │   TcpServer::newConnection()        │
                    │                                     │
                    │ 1. 轮询选择 IO 线程:                 │
                    │    ioLoop = threadPool_.getNextLoop()│
                    │                                     │
                    │ 2. 创建 TcpConnection:               │
                    │    conn = make_shared<TcpConnection>│
                    │                                     │
                    │ 3. 保存到 connections_ map          │
                    │                                     │
                    │ 4. 跨线程分发任务:                   │
                    │    ioLoop->runInLoop(                │
                    │        conn->connectEstablished)    │
                    └─────────────────────────────────────┘
                                       │
                                       │ 跨线程调用（如果 ioLoop != 主线程）
                                       ▼
                    ┌─────────────────────────────────────┐
                    │     EventLoop::runInLoop()          │
                    │                                     │
                    │ if (当前线程 == loop 所属线程) {     │
                    │     cb();  // 直接执行               │
                    │ } else {                            │
                    │     queueInLoop(cb);  // 入队       │
                    │ }                                   │
                    └─────────────────────────────────────┘
                                       │
                                       │ 入队 + 唤醒
                                       ▼
                    ┌─────────────────────────────────────┐
                    │     EventLoop::queueInLoop()        │
                    │                                     │
                    │ 1. mutex_.lock()                    │
                    │ 2. pendingFunctors_.push_back(cb)   │
                    │ 3. mutex_.unlock()                  │
                    │ 4. wakeup()  // 唤醒目标线程         │
                    └─────────────────────────────────────┘
                                       │
                                       ▼
                    ┌─────────────────────────────────────┐
                    │           IO 线程 (Sub Thread)       │
                    └─────────────────────────────────────┘
```

**关键代码实现 - 轮询分配:**
```cpp
// EventLoopThreadPool::getNextLoop()
EventLoop *EventLoopThreadPool::getNextLoop()
{
    EventLoop *loop = baseLoop_;  // 默认使用主线程的 loop

    if (!loops_.empty()) {
        // Round-Robin 轮询
        loop = loops_[next_];
        ++next_;
        if (static_cast<size_t>(next_) >= loops_.size()) {
            next_ = 0;  // 环形回绕
        }
    }

    return loop;
}
```

---

### 3.5 线程间通信机制 (eventfd + io_uring)

#### 3.5.1 eventfd 工作原理

`eventfd` 是 Linux 提供的一种轻量级线程间通信机制，内部维护一个 64 位无符号整数计数器：
- **写入**: 将写入的值累加到计数器
- **读取**: 返回计数器当前值并清零（非阻塞模式）

```
┌─────────────────────────────────────────────────────────────────────┐
│                         eventfd 通信机制                            │
│                                                                     │
│   主线程                                                 IO 线程    │
│   ┌─────────┐                                          ┌─────────┐ │
│   │         │                                          │         │ │
│   │ write() │ ───────── 写入 1 ──────────────────────▶ │ io_uring│ │
│   │ eventfd │                                          │ read()  │ │
│   │         │                eventfd                   │ 完成    │ │
│   └─────────┘              ┌───────┐                   └─────────┘ │
│                            │counter│                        │      │
│                            │  += 1 │                        ▼      │
│                            └───────┘               handleWakeup()  │
│                                                            │      │
│                                                            ▼      │
│                                                  doPendingFunctors()│
└─────────────────────────────────────────────────────────────────────┘
```

#### 3.5.2 唤醒流程 - 完整函数调用链

```
[主线程]                                    [IO 线程]
    │                                           │
    │ TcpServer::newConnection()                │
    │     │                                     │
    │     ▼                                     │
    │ ioLoop->runInLoop(cb)                     │
    │     │                                     │
    │     ▼                                     │
    │ EventLoop::queueInLoop(cb)                │
    │     │                                     │
    │     ├─▶ mutex_.lock()                     │
    │     ├─▶ pendingFunctors_.push_back(cb)    │
    │     ├─▶ mutex_.unlock()                   │
    │     │                                     │
    │     ▼                                     │
    │ EventLoop::wakeup()                       │
    │     │                                     │
    │     ▼                                     │
    │ ::write(wakeupFd_, &one, sizeof(one))     │
    │     │                                     │
    │     └──────────── eventfd ───────────────▶│ io_uring_wait_cqe() 返回
    │                                           │     │
    │                                           │     ▼
    │                                           │ EventLoop::handleCompletionEvent()
    │                                           │     │
    │                                           │     ▼
    │                                           │ IoType::Wakeup 分支
    │                                           │     │
    │                                           │     ▼
    │                                           │ EventLoop::handleWakeup()
    │                                           │     │
    │                                           │     ▼
    │                                           │ asyncReadWakeup() // 重新提交读请求
    │                                           │     │
    │                                           │     ▼
    │                                           │ EventLoop::doPendingFunctors()
    │                                           │     │
    │                                           │     ├─▶ mutex_.lock()
    │                                           │     ├─▶ functors.swap(pendingFunctors_)
    │                                           │     ├─▶ mutex_.unlock()
    │                                           │     │
    │                                           │     ▼
    │                                           │ for (func : functors) func();
    │                                           │     │
    │                                           │     ▼
    │                                           │ TcpConnection::connectEstablished()
```

**关键代码实现 - 唤醒机制:**
```cpp
// EventLoop::wakeup() - 唤醒目标线程
void EventLoop::wakeup()
{
    uint64_t one = 1;
    ssize_t n = ::write(wakeupFd_, &one, sizeof(one));
    if (n != sizeof(one)) {
        // 错误处理
    }
}

// EventLoop::handleWakeup() - 处理唤醒事件
void EventLoop::handleWakeup()
{
    // 重新提交读请求以监听下次唤醒
    asyncReadWakeup();
}

// EventLoop::doPendingFunctors() - 执行任务队列
void EventLoop::doPendingFunctors()
{
    std::vector<Functor> functors;
    callingPendingFunctors_ = true;

    {
        std::lock_guard<std::mutex> lock(mutex_);
        functors.swap(pendingFunctors_);  // 快速交换，减少锁持有时间
    }

    for (const Functor &functor : functors) {
        functor();
    }

    callingPendingFunctors_ = false;
}
```

---

### 3.6 事件循环主流程实现

```cpp
void EventLoop::loop()
{
    running_.store(true);
    quit_.store(false);

    while (!quit_.load()) {
        struct io_uring_cqe *cqe;
        
        // 阻塞等待完成事件
        int ret = io_uring_wait_cqe(&ring_, &cqe);
        
        if (ret < 0) {
            if (ret == -EINTR) continue;  // 被信号中断
            break;  // 其他错误
        }

        // 处理完成事件
        handleCompletionEvent(cqe);
        
        // 标记 CQE 已处理
        io_uring_cqe_seen(&ring_, cqe);
    }

    running_.store(false);
}

void EventLoop::handleCompletionEvent(struct io_uring_cqe *cqe)
{
    IoContext *ctx = static_cast<IoContext *>(io_uring_cqe_get_data(cqe));
    if (!ctx) return;

    int res = cqe->res;  // I/O 操作的结果

    switch (ctx->type) {
        case IoType::Accept:
        case IoType::Read:
        case IoType::Write:
            // 回调模式：调用绑定的 handler
            if (ctx->handler) {
                ctx->handler(res);
            }
            break;
            
        case IoType::Wakeup:
            // 唤醒事件
            handleWakeup();
            break;
    }

    // 协程模式：恢复挂起的协程
    if (ctx->coro_handle) {
        ctx->result_ = res;
        ctx->coro_handle.resume();
    }
}
```

---

### 3.7 连接关闭与资源回收

```
                    ┌─────────────────────────────────────┐
                    │        IO 线程 (处理连接)           │
                    │                                     │
                    │ 读取返回 0（对端关闭）或错误         │
                    │            │                        │
                    │            ▼                        │
                    │ TcpConnection::forceClose()         │
                    │            │                        │
                    │            ▼                        │
                    │ loop->queueInLoop(handleClose)      │
                    │            │                        │
                    │            ▼                        │
                    │ TcpConnection::handleClose()        │
                    │            │                        │
                    │            ▼                        │
                    │ closeCallback_(this)                │
                    └─────────────────────────────────────┘
                                       │
                                       │ 回调到 TcpServer
                                       ▼
                    ┌─────────────────────────────────────┐
                    │   TcpServer::removeConnection()     │
                    │                                     │
                    │ loop_->runInLoop([this, conn]() {   │
                    │     // 在主线程执行                   │
                    │     connections_.erase(conn->name); │
                    │                                     │
                    │     // 回到 IO 线程销毁连接          │
                    │     ioLoop->queueInLoop(            │
                    │         conn->connectDestroyed);    │
                    │ });                                 │
                    └─────────────────────────────────────┘
                                       │
                                       │ 分发到 IO 线程
                                       ▼
                    ┌─────────────────────────────────────┐
                    │ TcpConnection::connectDestroyed()   │
                    │                                     │
                    │ 1. 设置状态为 kDisconnected         │
                    │ 2. Socket 析构，close(fd)           │
                    │ 3. shared_ptr 引用计数归零          │
                    │ 4. TcpConnection 对象被销毁         │
                    └─────────────────────────────────────┘
```

---

## 4. 技术难点与设计亮点

### 4.1 线程安全的任务队列

```cpp
// 问题：多线程并发访问 pendingFunctors_
// 解决：使用 mutex 保护 + swap 技巧减少锁粒度

void EventLoop::queueInLoop(Functor cb)
{
    {
        std::lock_guard<std::mutex> lock(mutex_);
        pendingFunctors_.push_back(std::move(cb));
    }
    
    // 只有在以下情况才需要唤醒：
    // 1. 调用者不在当前线程
    // 2. 当前正在执行 pendingFunctors（防止遗漏新任务）
    if (!isInLoopThread() || callingPendingFunctors_) {
        wakeup();
    }
}

void EventLoop::doPendingFunctors()
{
    std::vector<Functor> functors;
    callingPendingFunctors_ = true;

    {
        // 快速交换，最小化锁持有时间
        std::lock_guard<std::mutex> lock(mutex_);
        functors.swap(pendingFunctors_);
    }

    // 在锁外执行回调，避免死锁和性能问题
    for (const Functor &functor : functors) {
        functor();
    }

    callingPendingFunctors_ = false;
}
```

### 4.2 eventfd 与 io_uring 结合

传统方案使用 `poll/epoll` 监听 eventfd，本项目创新性地使用 io_uring 统一管理：

```cpp
// EventLoop 构造函数中
void EventLoop::asyncReadWakeup()
{
    struct io_uring_sqe *sqe = io_uring_get_sqe(&ring_);
    if (sqe) {
        io_uring_prep_read(sqe, wakeupFd_, &wakeupBuffer_, sizeof(wakeupBuffer_), 0);
        io_uring_sqe_set_data(sqe, &wakeupContext_);
        io_uring_submit(&ring_);
    }
}
```

**优势**：
1. 统一的事件处理模型，代码更简洁
2. 减少系统调用次数
3. 利用 io_uring 的批处理能力

### 4.3 连接生命周期管理

使用 `shared_ptr` 配合回调链，确保连接在处理过程中不会被意外销毁：

```cpp
void TcpConnection::handleClose()
{
    // guard 防止在回调过程中 this 被销毁
    std::shared_ptr<TcpConnection> guard(shared_from_this());
    
    if (closeCallback_) {
        closeCallback_(guard);  // 传递 shared_ptr
    }
}
```

---

## 5. 性能优化要点总结

| 优化点                | 说明                                          |
|----------------------|-----------------------------------------------|
| One Loop Per Thread  | 避免锁竞争，每个线程独立处理自己的连接          |
| Round-Robin 分配     | 简单高效的负载均衡策略                         |
| swap 技巧            | 减少锁持有时间，提高并发性能                    |
| io_uring 批处理      | 多个 SQE 可以一次 submit，减少系统调用          |
| eventfd 轻量唤醒     | 比 pipe 更轻量，只需要一个 fd                  |
| 栈上 EventLoop       | 避免动态内存分配                               |

---

## 6. 总结

本项目的主从线程任务分发机制可以概括为：

1. **主线程**：运行 Acceptor，监听新连接，通过 Round-Robin 将新连接分发给 IO 线程
2. **IO 线程**：每个线程运行独立的 EventLoop，处理分配给自己的连接的读写操作
3. **线程间通信**：通过 eventfd + io_uring 实现，主线程写 eventfd 唤醒 IO 线程
4. **任务队列**：跨线程任务通过 `queueInLoop` 入队，被唤醒后由 `doPendingFunctors` 执行

这种设计充分利用了多核 CPU，同时通过 io_uring 的异步 I/O 能力，实现了高性能的网络服务器架构。